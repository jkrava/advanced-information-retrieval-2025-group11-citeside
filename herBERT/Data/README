
# Argument from Paper A to Paper A

**Paper 1:** 2020.acl-main.398

    title: {T}a{P}as: Weakly Supervised Table Parsing via Pre-training

    authors: Herzig, Jonathan  and
    Nowak, Pawel Krzysztof  and M{\"u}ller, Thomas  and Piccinno, Francesco  and Eisenschlos, Julian
    year: 2020

**Paper 2:** P19-1459

    title: Probing Neural Network Comprehension of Natural Language Arguments
    authors: Niven, Timothy  and Kao, Hung-Yu
    year: 2019

**Paper 3:** D19-1284

    A Discrete Hard {EM} Approach for Weakly Supervised Question Answering
    authors: Min, Sewon  and Chen, Danqi  and Hajishirzi Hannaneh  and Zettlemoyer, Luke
    year: 2019

**Paper 4:** 2020.emnlp-main.213

    title: {COMET}: A Neural Framework for {MT} Evaluation
    authors: Rei, Ricardo  and
    Stewart, Craig  and
    Farinha, Ana C  and
    Lavie, Alon
    year: 2020


**Paper 5:**  N19-1423
    
    {BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding
    authors: Devlin, Jacob  and Chang, Ming-Wei  and
    Lee, Kenton  and Toutanova, Kristina
    year: 2019

## A -> B 

### Sample 1: check citation from Paper 1 in Paper 2
2020.acl-main.398 -> P19-1459

**citation**: "We follow Niven and Kao (2019) and report the median for 5 independent runs, as BERT-based models can degenerate."

**argument**: "Since Bert based models can degenerate it is a practice to report the median of indepent runs. Niven and Kao (2019)" 

### Sample 2: check citation from Paper 1 in Paper 3
2020.acl-main.398 -> N19-1423

**citation**: Concretely, we set the supervision to be of cell selection if p a (op 0 ) â‰¥ S, where 0 < S < 1 is a threshold hyperparameter, and the scalar answer supervision otherwise. This follows hard EM (Min et al., 2019), as for spurious programs we pick the most probable one according to the current model.

**argument**: Hard EM commits to a models most probable program by switching supervision if the probability of a specific operation exceeds a certain threshold. (Min et al., 2019)

### Sample 3: check citation from Paper 4 in Paper 5
2020.emnlp-main.213 -> N19-1423

**citation**: In recent years, word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019) have emerged as a commonly used alternative to n-gram matching for capturing word semantics similarity.

**argument**: Word embeddings are commonly used for capturing word semantivs similarity. (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019)

